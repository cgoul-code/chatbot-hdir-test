{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generate_joke: Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n",
      "Response improve_joke: Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse and make sure it didn’t click away!\n",
      "Response final_joke: Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse... and then it realized it could just order a pizza online instead!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# LLM og verktøy\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core import (StorageContext,  load_index_from_storage)\n",
    "\n",
    "# Indeksverktøy\n",
    "LLMGPT4omini = AzureOpenAI(\n",
    "    model=os.getenv('AZURE_OPENAI_MODEL_GPT4omini'),\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME_GPT4omini'),\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME_GPT4omini'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY_GPT4omini'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_AZURE_ENDPOINT_GPT4omini'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSJON_GPT4omini'),\n",
    "    temperature=0.0,\n",
    "    timeout= 120,\n",
    ")\n",
    "\n",
    "def read_index_from_storage(storage):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage)\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "# Sett Azure OpenAI-legitimasjon\n",
    "\n",
    "llm = LLMGPT4omini\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "   \n",
    "def generate_joke(state: State) :\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpfull assistant\" ),\n",
    "        ChatMessage(role=\"user\", content=f\"Write a short joke about {state['topic']}\")  ]\n",
    "    response = llm.chat(messages)\n",
    "    print(f'Response generate_joke: {response.message.content}')\n",
    "\n",
    "    return {'joke': response.message.content}\n",
    "\n",
    "def improve_joke(state: State) :\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpfull assistant\" ),\n",
    "        ChatMessage(role=\"user\", content=f\"Make that joke funnier by adding wordplay: {state['joke']}\")  ]\n",
    "    response = llm.chat(messages)\n",
    "    print(f'Response improve_joke: {response.message.content}')\n",
    "\n",
    "    return {'improved_joke': response.message.content}\n",
    "\n",
    "def polish_joke(state: State) :\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpfull assistant\" ),\n",
    "        ChatMessage(role=\"user\", content=f\"Make a surprising twist to this joke: {state['improved_joke']}\")  ]\n",
    "    response = llm.chat(messages)\n",
    "    print(f'Response final_joke: {response.message.content}')\n",
    "\n",
    "    return {'final_joke': response.message.content}\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    if \"?\" in state['joke'] or \"!\" in state['joke']:\n",
    "        return \"Pass\"\n",
    "    else:\n",
    "        return \"Fail\"\n",
    "    \n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "#build workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "workflow.add_node(\"improve_joke\", improve_joke)\n",
    "workflow.add_node(\"polish_joke\", polish_joke)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"generate_joke\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_joke\", check_punchline, {\"Pass\": \"improve_joke\", \"Fail\": END}\n",
    ")\n",
    "workflow.add_edge(\"improve_joke\", \"polish_joke\")\n",
    "workflow.add_edge(\"polish_joke\", END)\n",
    "\n",
    "# Compile\n",
    "chain = workflow.compile()\n",
    "\n",
    "# Show workflow\n",
    "#display(Image(chain.get_graph().draw_mermaid_png()))\n",
    "\n",
    "state = chain.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
