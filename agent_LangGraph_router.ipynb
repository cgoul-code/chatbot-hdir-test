{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f3582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mermaid diagram saved to: graph.mmd\n",
      "üåê Opening https://mermaid.live - paste your diagram code there.\n",
      "\n",
      "llm_call_1 invoked: Svarer p√• sp√∏rsm√•l om tobakk\n",
      "1 text chunks after repacking\n",
      "\u001b[33mAnswer: \u001b[34mHer er noen tips for √• slutte med snus:\n",
      "1. Finn noe som motiverer deg til √• slutte, for eksempel √• spare penger til en spesifikk ting.\n",
      "2. Sett en sluttdato og v√¶r mentalt forberedt p√• abstinenser.\n",
      "3. Finn st√∏tte i andre, del dine planer med venner eller bruk Slutta-appen for motivasjon.\n",
      "4. Legg en plan for √• h√•ndtere fristelser og tilbakefall, og bruk alternative metoder som tyggegummi eller drops.\n",
      "5. Bel√∏nn deg selv n√•r du n√•r delm√•l, for eksempel med en kinobillett eller en fin middag. Lykke til med snusslutt!\n",
      "\n",
      "llm_call_2 invoked: Svarer p√• sp√∏rsm√•l om prevensjon\n",
      "1 text chunks after repacking\n",
      "\u001b[33mAnswer: \u001b[34mVanlige bivirkninger av p-piller inkluderer hodepine, kvalme, kviser, redusert sexlyst, √∏mme bryster og hum√∏rsvingninger.\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "# LLM og verkt√∏y\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core import (get_response_synthesizer)\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import (VectorStoreIndex, StorageContext,  load_index_from_storage)\n",
    "\n",
    "# Import av embedding-moduler\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbeddingModelType\n",
    "\n",
    "# Indeksverkt√∏y\n",
    "LLMGPT4omini = AzureChatOpenAI(\n",
    "    model=os.getenv('AZURE_OPENAI_MODEL_GPT4omini'),\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME_GPT4omini'),\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME_GPT4omini'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY_GPT4omini'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_AZURE_ENDPOINT_GPT4omini'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSJON_GPT4omini'),\n",
    "    temperature=0.0,\n",
    "    timeout= 120,\n",
    ")\n",
    "\n",
    "def read_index_from_storage(storage):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage)\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "# Sett Azure OpenAI-legitimasjon\n",
    "\n",
    "llm = LLMGPT4omini\n",
    "\n",
    "\n",
    "\n",
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"contraception\", \"tobacco\", \"mental health\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "# State\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "    index_tobakk: VectorStoreIndex\n",
    "    index_prevensjon : VectorStoreIndex\n",
    "    index_psykiskhelse : VectorStoreIndex\n",
    "    \n",
    "    \n",
    "# some global variables\n",
    "chat_text_qa_msgs = [\n",
    "        ChatMessage(\n",
    "            role=MessageRole.SYSTEM,\n",
    "            content=( \n",
    "                \"You are a helpful assistant, and you will be given a user request.\"\n",
    "                \"Some rules to follow:\"\n",
    "                \"- Always answer the request using the given context information and not prior knowledge\"\n",
    "                \"- Always answer in norwegian\"\n",
    "                \"- Always answer using the format: '\\033[33mAnswer: \\033[34m<answer>\\033[0m'\"\n",
    "\n",
    "            ),\n",
    "        )\n",
    "        ,\n",
    "        ChatMessage(\n",
    "            role=MessageRole.USER,\n",
    "            content=(\n",
    "                \"Context information is below.\\n\"\n",
    "                \"---------------------\\n\"\n",
    "                \"{context_str}\\n\"\n",
    "                \"---------------------\\n\"\n",
    "                \"Query: {query_str}\\n\"\n",
    "                \"Answer: \"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "text_qa_template =  ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode= \"tree_summarize\",\n",
    "    text_qa_template = text_qa_template,\n",
    "    summary_template= text_qa_template, #definitly in use for response_mode = tree_summarize\n",
    "    structured_answer_filtering=True, \n",
    "    verbose=True,\n",
    ")\n",
    "text_splitter = SentenceSplitter.from_defaults(chunk_size=1024, chunk_overlap=75)\n",
    "\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call_1(state: State):\n",
    "    \"\"\"Svarer p√• sp√∏rsm√•l om tobakk\"\"\"\n",
    "    print(\"\\nllm_call_1 invoked: Svarer p√• sp√∏rsm√•l om tobakk\")\n",
    "    storage = './blobstorage/chatbot/ungnotobakk'\n",
    "    if not state.get(\"index_tobakk\"):\n",
    "        state[\"index_tobakk\"] = read_index_from_storage(storage)\n",
    "    else:\n",
    "        print(f'Index for tobakk already loaded from {storage}')\n",
    "\n",
    "    query_engine = state[\"index_tobakk\"].as_query_engine(\n",
    "        similarity_cutoff=0.7, \n",
    "        similarity_top_k=10,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(state[\"input\"])\n",
    "    return {\"output\": response}\n",
    "\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"Svarer p√• sp√∏rsm√•l om prevensjon\"\"\"\n",
    "    print(\"\\nllm_call_2 invoked: Svarer p√• sp√∏rsm√•l om prevensjon\")\n",
    "    storage = './blobstorage/chatbot/prevensjon'\n",
    "    if not state.get(\"index_prevensjon\"):\n",
    "        state[\"index_prevensjon\"] = read_index_from_storage(storage)\n",
    "    else:\n",
    "        print(f'Index for prevensjon already loaded from {storage}')\n",
    "\n",
    "    query_engine = state[\"index_prevensjon\"].as_query_engine(\n",
    "        similarity_cutoff=0.7, \n",
    "        similarity_top_k=10,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(state[\"input\"])\n",
    "    return {\"output\": response}\n",
    "\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"Svarer p√• sp√∏rsm√•l om psykiskhelse\"\"\"\n",
    "    print(\"\\nllm_call_3 invoked: Svarer p√• sp√∏rsm√•l om psykiskhelse\")\n",
    "    storage = './blobstorage/chatbot/psykiskhelse'\n",
    "    if not state.get(\"index_psykiskhelse\"):\n",
    "        state[\"index_psykiskhelse\"] = read_index_from_storage(storage)\n",
    "    else:\n",
    "        print(f'Index for psykiskhelse already loaded from {storage}')\n",
    "\n",
    "    query_engine = state[\"index_psykiskhelse\"].as_query_engine(\n",
    "        similarity_cutoff=0.7, \n",
    "        similarity_top_k=10,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(state[\"input\"])\n",
    "    return {\"output\": response}\n",
    "\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to contraception, tobacco, mental health, based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"decision\": decision.step}\n",
    "\n",
    "# Conditional edge function to route to the appropriate node\n",
    "def route_decision(state: State):\n",
    "    # Return the node name you want to visit next\n",
    "    if state[\"decision\"] == \"tobacco\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"contraception\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"mental health\":\n",
    "        return \"llm_call_3\"\n",
    "\n",
    "    \n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {  # Name returned by route_decision : Name of next node to visit\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ")\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# Compile workflow\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "# Show the workflow\n",
    "# display(Image(router_workflow.get_graph().draw_mermaid_png(\n",
    "#     curve_style=CurveStyle.LINEAR,\n",
    "#     node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
    "#     wrap_label_n_words=9,\n",
    "#     background_color=\"white\",\n",
    "#     padding=10\n",
    "#     ,)))\n",
    "from graph_utils import save_mermaid_diagram\n",
    "save_mermaid_diagram(router_workflow.get_graph())\n",
    "\n",
    "\n",
    "initial_state = {\n",
    "    \"input\": \"Gi meg noen tips om hvordan slutte med snus\",\n",
    "    \"decision\": \"\",\n",
    "    \"output\": \"\",\n",
    "    \"index_tobakk\": None,\n",
    "    \"index_prevensjon\": None,\n",
    "    \"index_psykiskhelse\": None\n",
    "}\n",
    "state = router_workflow.invoke(initial_state)\n",
    "\n",
    "# Invoke\n",
    "print(state[\"output\"])\n",
    "\n",
    "initial_state = {\n",
    "    \"input\": \"Hvilke bivirkninger har p-piller?\",\n",
    "    \"decision\": \"\",\n",
    "    \"output\": \"\",\n",
    "    \"index_tobakk\": None,\n",
    "    \"index_prevensjon\": None,\n",
    "    \"index_psykiskhelse\": None\n",
    "}\n",
    "state = router_workflow.invoke(initial_state)\n",
    "\n",
    "# Invoke\n",
    "print(state[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
