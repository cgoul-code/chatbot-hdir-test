{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa72ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mermaid diagram saved to: graph.mmd\n",
      "üåê Opening https://mermaid.live - paste your diagram code there.\n",
      "1 text chunks after repacking\n",
      "1 text chunks after repacking\n",
      "1 text chunks after repacking\n",
      "\u001b[33mQuery: \u001b[34mN√•r sank Titanic?\u001b[0m\u001b[33mAnswer: \u001b[34mDette sp√∏rsm√•let omhandler ikke informasjonen som er gitt i konteksten.\n",
      "\u001b[33mQuery: \u001b[34mEr snus skadelig for min helse?\u001b[0m\u001b[33mAnswer: \u001b[34mJa, snus er skadelig for helsen din. Snus inneholder helseskadelige stoffer og kan f√∏re til ulike helseproblemer som √∏kt risiko for kreft, p√•virkning av hjernen, hjerteproblemer, diabetes og tann- og munnhelseproblemer. Det er spesielt viktig √• v√¶re forsiktig med snus under graviditet da det kan p√•virke b√•de fosteret og den gravide p√• flere m√•ter.\n",
      "\u001b[33mQuery: \u001b[34mKan du gi meg tips p√• hvordan slutte √• r√∏yke?\u001b[0m\u001b[33mAnswer: \u001b[34mFor √• slutte √• r√∏yke kan du ta i bruk f√∏lgende tips:\n",
      "\n",
      "1. Finn noe som motiverer deg til √• slutte. Skriv ned alle gode grunner for √• slutte og ha de synlige hver dag.\n",
      "2. Sett en sluttdato og ta hensyn til abstinenser i tiden etter nikotinslutt.\n",
      "3. Finn st√∏tte i andre, del med andre at du har bestemt deg for √• slutte.\n",
      "4. Bruk Slutta-appen for daglige motivasjonsmeldinger og tips.\n",
      "5. Legg en plan mot fristelser og tilbakefall, tenk gjennom ulike situasjoner p√• forh√•nd og lag en plan for √• unng√• tilbakefall.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Tips for Quitting Smoking\n",
       "\n",
       "For √• slutte √• r√∏yke kan du ta i bruk f√∏lgende tips:\n",
       "\n",
       "1. **Finn motivasjon**: Skriv ned alle gode grunner for √• slutte og ha dem synlige hver dag.\n",
       "2. **Sett en sluttdato**: V√¶r oppmerksom p√• abstinenser i tiden etter nikotinslutt.\n",
       "3. **S√∏k st√∏tte**: Del med andre at du har bestemt deg for √• slutte, og finn st√∏tte i venner og familie.\n",
       "4. **Bruk apper**: Bruk Slutta-appen for daglige motivasjonsmeldinger og tips.\n",
       "5. **Lag en plan**: Tenk gjennom ulike situasjoner som kan friste deg til √• r√∏yke, og lag en plan for √• unng√• tilbakefall.\n",
       "\n",
       "---\n",
       "\n",
       "# Er Snus Skadelig for Helsen?\n",
       "\n",
       "Ja, snus er skadelig for helsen din. Snus inneholder helseskadelige stoffer og kan f√∏re til ulike helseproblemer, inkludert:\n",
       "\n",
       "- √òkt risiko for kreft\n",
       "- P√•virkning av hjernen\n",
       "- Hjerteproblemer\n",
       "- Diabetes\n",
       "- Tann- og munnhelseproblemer\n",
       "\n",
       "Det er spesielt viktig √• v√¶re forsiktig med snus under graviditet, da det kan p√•virke b√•de fosteret og den gravide p√• flere m√•ter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from typing_extensions import Annotated, List\n",
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# LLM og verkt√∏y\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core import (get_response_synthesizer)\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import (VectorStoreIndex, StorageContext,  load_index_from_storage)\n",
    "from llama_index.core.query_engine import BaseQueryEngine\n",
    "\n",
    "# Import av langchain og langgraph\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Indeksverkt√∏y\n",
    "LLMGPT4omini = AzureChatOpenAI(\n",
    "    model=os.getenv('AZURE_OPENAI_MODEL_GPT4omini'),\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME_GPT4omini'),\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME_GPT4omini'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY_GPT4omini'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_AZURE_ENDPOINT_GPT4omini'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSJON_GPT4omini'),\n",
    "    temperature=0.0,\n",
    "    timeout= 120,\n",
    ")\n",
    "\n",
    "def read_index_from_storage(storage):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage)\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "# Sett Azure OpenAI-legitimasjon\n",
    "\n",
    "llm = LLMGPT4omini\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=(\n",
    "            \"You are a helpful assistant, and you will be given a user request.\\n\"\n",
    "            \"Some rules to follow:\\n\"\n",
    "            \"- Always answer the request using the given context information and not prior knowledge.\\n\"\n",
    "            \"- Always answer in norwegian.\\n\"\n",
    "        ),\n",
    "    )\n",
    "    ,\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "text_qa_template =  ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode= \"tree_summarize\",\n",
    "    text_qa_template = text_qa_template,\n",
    "    summary_template= text_qa_template, #definitly in use for response_mode = tree_summarize\n",
    "    structured_answer_filtering=True, \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# intialize the LLM engine:\n",
    "storage = './blobstorage/chatbot/ungnotobakk'\n",
    "index = read_index_from_storage(storage)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_cutoff=0.7,\n",
    "    similarity_top_k=10,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class SubQuery(BaseModel):\n",
    "    subquery: str = Field(\n",
    "        description=\"The subquery\",\n",
    "    )\n",
    "    answer: str = Field(\n",
    "        description=\"Answer to the subquery\",\n",
    "    )\n",
    "\n",
    "\n",
    "class SubQueries(BaseModel):\n",
    "    subqueries: List[SubQuery] = Field(\n",
    "        description=\"Sections of the structured answer.\",\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(SubQueries)\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    query_engine: BaseQueryEngine\n",
    "    query: str  # Report topic\n",
    "    subqueries: list[SubQuery]  # List of subqueries\n",
    "    completed_answers: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel (That operator.add means: take the completed_answers lists from each worker and append them together.)\n",
    "    final_answer: str  # Final report\n",
    "    \n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    subquery: SubQuery\n",
    "    completed_answers: Annotated[list, operator.add] \n",
    "    \n",
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for solving the question\"\"\"\n",
    "    # Generate queries\n",
    "    report_queries = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Refrase the user query generating a subquery in norwegian. If the user query har several queries, generate several subqueries. Do not answer the subqueries\"),\n",
    "            HumanMessage(content=f\"Here is query from a user: {state['query']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"subqueries\": report_queries.subqueries}\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker answers a subquery using the relevant index\"\"\"\n",
    "\n",
    "    response_obj = query_engine.query(state['subquery'].subquery)\n",
    "    state['subquery'].answer = response_obj.response\n",
    "\n",
    "    s = \"\\033[33mQuery: \\033[34m\" + state['subquery'].subquery + \"\\033[0m\" + response_obj.response\n",
    "    print(s)\n",
    "\n",
    "    return {\"completed_answers\": [state['subquery'].subquery + \"\\n\" + response_obj.response]}\n",
    "\n",
    "    \n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full answer from answers from the subqueries\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_answers = state[\"completed_answers\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_answers = \"\\n\\n---\\n\\n\".join(completed_answers)\n",
    "    \n",
    "    aggregated_answer = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"from this list of answers, reorganize a final answer. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the list of answers: {completed_report_answers}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #print(aggregated_answer.content)\n",
    "\n",
    "    return {\"final_answer\": aggregated_answer.content}\n",
    "\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"subquery\": s, \"query_engine\": query_engine}) for s in state[\"subqueries\"]]\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "\n",
    "\n",
    "from graph_utils import save_mermaid_diagram\n",
    "save_mermaid_diagram(orchestrator_worker.get_graph())\n",
    "# Show the workflow\n",
    "#display(Image(router_workflow.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = orchestrator_worker.invoke({\"query\": \"Kan du gi meg tips p√• hvordan slutte√• r√∏yke? N√•r sank titanic? Er snus skadelig for min helse?\",\n",
    "                                    \"query_engine\": query_engine})\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
